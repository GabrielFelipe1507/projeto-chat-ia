import streamlit as st
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import AIMessage, HumanMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables.history import RunnableWithMessageHistory
# ChatMessageHistory ainda √© √∫til para *estruturar* a mem√≥ria antes de pass√°-la para a chain
from langchain_community.chat_message_histories import ChatMessageHistory 
import os
from dotenv import load_dotenv
import time # Para adicionar um pequeno delay e melhorar a percep√ß√£o

# Importa as fun√ß√µes do nosso backend de banco de dados
from db import (
    # criar_tabelas, # N√£o precisa chamar sempre, o db.py j√° faz isso se rodado diretamente
    listar_conversas, 
    criar_nova_conversa, 
    carregar_mensagens, 
    salvar_mensagem
)

# Carrega as vari√°veis de ambiente (.env)
load_dotenv()

# --- 1. CONFIGURA√á√ÉO DO LLM (Sem mudan√ßas) ---
try:
    llm = ChatGoogleGenerativeAI(model="models/gemini-2.5-flash-preview-09-2025", # Usando o nome que funcionou
                                 google_api_key=os.getenv("GEMINI_API_KEY"),
                                 convert_system_message_to_human=True)
except Exception as e:
    st.error(f"Erro ao inicializar o LLM: {e}. Verifique sua chave de API e conex√£o.")
    st.stop() # Interrompe a execu√ß√£o se o LLM n√£o puder ser carregado

# --- 2. CONFIGURA√á√ÉO DO BACKEND (Agora com Mem√≥ria do Banco) ---

# Fun√ß√£o MODIFICADA para buscar o hist√≥rico DO BANCO DE DADOS
# Esta fun√ß√£o √© crucial para o RunnableWithMessageHistory saber de onde ler/escrever
# session_id aqui ser√° o ID da conversa no banco
def get_session_history(session_id):
    if session_id is None: # Se n√£o h√° ID (novo chat), retorna hist√≥rico vazio
        # print("DEBUG: get_session_history chamado com session_id None. Retornando hist√≥rico vazio.")
        return ChatMessageHistory() 
        
    # Usa o session_id (que ser√° o ID da conversa no banco) para carregar
    # Retorna uma lista de HumanMessage ou AIMessage
    # print(f"DEBUG: get_session_history tentando carregar mensagens para id_conversa: {session_id}")
    mensagens_do_banco = carregar_mensagens(session_id) 
    
    # Cria um objeto ChatMessageHistory em mem√≥ria e popula com as mensagens do banco
    # A chain espera receber um objeto deste tipo
    history = ChatMessageHistory()
    for msg in mensagens_do_banco:
        history.add_message(msg)
    # print(f"DEBUG: Hist√≥rico carregado para session_id {session_id}: {len(history.messages)} mensagens.")
    return history

# N√ÉO precisamos mais inicializar st.session_state.chat_histories aqui
# A mem√≥ria agora vive no banco de dados

# Cria o template do prompt (Sem mudan√ßas)
prompt_template = ChatPromptTemplate.from_messages(
    [
        ("system", "Voc√™ √© um assistente prestativo. Responda √†s perguntas do usu√°rio da forma mais completa e educada poss√≠vel."),
        MessagesPlaceholder(variable_name="history"), 
        ("human", "{input}"), 
    ]
)

# Cria a "Chain" moderna com mem√≥ria (Sem mudan√ßas na estrutura)
# Agora, get_session_history vai buscar no banco!
try:
    chain_with_memory = RunnableWithMessageHistory(
        prompt_template | llm, 
        get_session_history,    
        input_messages_key="input", 
        history_messages_key="history", 
    )
except Exception as e:
    st.error(f"Erro ao criar a chain com mem√≥ria: {e}")
    st.stop()

# --- 3. CONFIGURA√á√ÉO DO FRONTEND (Streamlit com Sidebar e Estado) ---
st.set_page_config(page_title="Chatbot com MySQL", layout="wide")
st.title("Meu Chatbot com Gemini e MySQL üíæü§ñ")

# --- Barra Lateral (Sidebar) para Gerenciar Conversas ---
st.sidebar.title("Minhas Conversas")

# Bot√£o para iniciar um novo chat
if st.sidebar.button("‚ûï Novo Chat", key="novo_chat_sidebar_button"):
    print("DEBUG: Bot√£o Novo Chat clicado.")
    # Limpa o ID da conversa ativa para indicar que queremos um novo chat
    st.session_state.conversa_ativa_id = None
    # For√ßa o recarregamento da p√°gina para limpar a tela principal
    st.rerun() 

# Listar conversas existentes do banco
try:
    # print("DEBUG: Listando conversas do banco...")
    lista_de_conversas = listar_conversas() 
    # print(f"DEBUG: Conversas encontradas: {lista_de_conversas}")
except Exception as e:
    st.sidebar.error(f"Erro ao listar conversas: {e}")
    lista_de_conversas = []

# Garante que temos um estado para a conversa ativa
# Se n√£o houver nenhum ID salvo, come√ßa como None
if "conversa_ativa_id" not in st.session_state:
    st.session_state.conversa_ativa_id = None 
    # print("DEBUG: st.session_state.conversa_ativa_id inicializado como None.")

# Mostra bot√µes para cada conversa existente
st.sidebar.divider() # Adiciona uma linha divis√≥ria
st.sidebar.markdown("**Hist√≥rico:**")
if not lista_de_conversas:
    st.sidebar.info("Nenhuma conversa ainda.")
else:
    # Mostra primeiro as conversas mais recentes (j√° ordenado em listar_conversas)
    for conversa in lista_de_conversas: 
        # Usa o ID da conversa como chave √∫nica para o bot√£o
        # Isso garante que o Streamlit saiba qual bot√£o foi clicado
        # Usamos .get() para seguran√ßa caso 'titulo' n√£o exista
        titulo_display = conversa.get('titulo', f'Conversa ID {conversa["id"]}') 
        if st.sidebar.button(titulo_display, key=f"conversa_{conversa['id']}", use_container_width=True):
            print(f"DEBUG: Bot√£o da conversa {conversa['id']} clicado.")
            # Ao clicar, define este como o ID da conversa ativa
            st.session_state.conversa_ativa_id = conversa['id']
            # Recarrega a p√°gina para mostrar o hist√≥rico da conversa selecionada
            st.rerun() 

# --- √Årea Principal do Chat ---

# Verifica se h√° uma conversa ativa selecionada
active_chat_id = st.session_state.get("conversa_ativa_id") # Usar .get() √© mais seguro
# print(f"DEBUG: ID da conversa ativa no in√≠cio da renderiza√ß√£o: {active_chat_id}")

if active_chat_id:
    
    # Mostra um t√≠tulo indicando qual conversa est√° ativa (opcional)
    # st.subheader(f"Conversa ID: {active_chat_id}")
    
    # Carrega o hist√≥rico da conversa ativa do banco para EXIBI√á√ÉO
    # A chain usa get_session_history internamente, mas precisamos buscar de novo 
    # para mostrar as mensagens na tela antes do novo input.
    try:
        # print(f"DEBUG: Carregando hist√≥rico para exibi√ß√£o (ID: {active_chat_id})")
        chat_history_para_exibir = get_session_history(active_chat_id)
        # print(f"DEBUG: Hist√≥rico para exibi√ß√£o carregado: {len(chat_history_para_exibir.messages)} mensagens.")
    except Exception as e:
        st.error(f"Erro ao carregar hist√≥rico para exibi√ß√£o: {e}")
        chat_history_para_exibir = ChatMessageHistory() # Mostra vazio em caso de erro

    # Exibe o hist√≥rico carregado
    for message in chat_history_para_exibir.messages:
        role = "ai" if isinstance(message, AIMessage) else "human"
        with st.chat_message(role):
            st.markdown(message.content)

    # Pega a nova mensagem do usu√°rio
    if prompt := st.chat_input("Digite sua mensagem..."):
        print(f"DEBUG: Usu√°rio digitou: {prompt}")
        
        # 1. (Frontend) Mostra a mensagem do usu√°rio na tela
        with st.chat_message("human"):
            st.markdown(prompt)
        
        # Salva a mensagem humana no banco PRIMEIRO
        if not salvar_mensagem(active_chat_id, "human", prompt):
             st.error("Erro ao salvar sua mensagem no banco de dados.")
             st.stop() # Interrompe se n√£o conseguir salvar a mensagem do usu√°rio
        # else: # S√≥ continua se salvou com sucesso 
        
        # 2. (Backend/LLM) Pensa na resposta 
        # A chain usa get_session_history para pegar o hist√≥rico atualizado (incluindo o prompt acima)
        try:
            # Exibe um spinner enquanto o LLM pensa
            with st.spinner("Digitando..."):
                print(f"DEBUG: Chamando chain.invoke para conversa {active_chat_id}")
                response = chain_with_memory.invoke(
                    {"input": prompt},
                    # Passa o ID da conversa para a chain saber qual hist√≥rico usar/atualizar
                    config={"configurable": {"session_id": active_chat_id}} 
                )
                print(f"DEBUG: Resposta recebida do LLM: {response}")

            # 3. (Frontend) Mostra a resposta da IA na tela (apenas se n√£o houve erro)
            if response and hasattr(response, 'content'):
                with st.chat_message("ai"):
                    st.markdown(response.content)
                    
                # Salva a mensagem da IA no banco DEPOIS que ela foi recebida e exibida
                if not salvar_mensagem(active_chat_id, "ai", response.content):
                      st.error("Erro ao salvar a resposta da IA no banco de dados.")
                
                # N√£o precisa recarregar aqui, o Streamlit atualiza a tela
                # st.rerun() # Remover ou comentar esta linha

            else:
                st.error("O LLM n√£o retornou uma resposta v√°lida.")


        except Exception as e:
            st.error(f"Erro ao chamar o LLM ou salvar a resposta: {e}")
            # Considerar n√£o fazer st.rerun() aqui para o usu√°rio ver o erro

else:
    # Se nenhuma conversa est√° selecionada, mostra uma mensagem inicial
    st.info("‚¨ÖÔ∏è Selecione uma conversa na barra lateral ou clique em '‚ûï Novo Chat' para come√ßar.")
    
    # Caixa de input para iniciar um NOVO chat
    if prompt := st.chat_input("Digite sua primeira mensagem para iniciar um novo chat...", key="novo_chat_input"):
         print(f"DEBUG: Usu√°rio digitou a primeira mensagem: {prompt}")
         # Cria uma nova conversa no banco ANTES de enviar a primeira mensagem
         try:
             print("DEBUG: Tentando criar nova conversa no banco...")
             novo_id = criar_nova_conversa()
         except Exception as e:
             st.error(f"Erro ao criar nova conversa no banco: {e}")
             novo_id = None
             
         if novo_id:
             st.session_state.conversa_ativa_id = novo_id # Define como ativa
             print(f"DEBUG: Nova conversa criada com ID {novo_id}. Definido como ativa.")
             
             # Salva a primeira mensagem humana
             if not salvar_mensagem(novo_id, "human", prompt):
                  st.error("Erro ao salvar sua primeira mensagem.")
             # else: # S√≥ continua se salvou
             # Pensa na resposta (a chain vai carregar o hist√≥rico vazio + a 1a msg humana)
             try:
                 # Exibe um spinner enquanto o LLM pensa
                 with st.spinner("Digitando..."):
                     print(f"DEBUG: Chamando chain.invoke pela primeira vez para conversa {novo_id}")
                     response = chain_with_memory.invoke(
                         {"input": prompt},
                         config={"configurable": {"session_id": novo_id}} 
                     )
                     print(f"DEBUG: Primeira resposta recebida do LLM: {response}")
                 
                 # Salva a primeira resposta da IA (apenas se v√°lida)
                 if response and hasattr(response, 'content'):
                     if not salvar_mensagem(novo_id, "ai", response.content):
                           st.error("Erro ao salvar a primeira resposta da IA.")
                     
                     # Recarrega a p√°gina para mostrar o novo chat com as mensagens
                     # e atualizar a sidebar
                     print("DEBUG: Recarregando a p√°gina (st.rerun) ap√≥s a primeira intera√ß√£o.")
                     time.sleep(0.5) # Pequeno delay para garantir que o salvamento no DB conclua
                     st.rerun() 
                 else:
                      st.error("O LLM n√£o retornou uma resposta v√°lida para a primeira mensagem.")

             except Exception as e:
                  st.error(f"Erro ao chamar o LLM para a primeira mensagem: {e}")
         else:
             st.error("N√£o foi poss√≠vel criar uma nova conversa no banco.")
