import streamlit as st
from langchain_google_genai import ChatGoogleGenerativeAI
# Importa칞칫es CORRETAS para a mem칩ria na vers칚o > 1.0
from langchain_core.messages import AIMessage, HumanMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_community.chat_message_histories import ChatMessageHistory # Para a mem칩ria em RAM
import os
from dotenv import load_dotenv

# Carrega as vari치veis de ambiente (sua chave GEMINI_API_KEY) do arquivo .env
load_dotenv()

# --- 1. CONFIGURA칂츾O DO LLM (Conex칚o LLM - 30 min) ---
# Inicializa o modelo Gemini (sem altera칞칫es aqui)
llm = ChatGoogleGenerativeAI(model="models/gemini-2.5-flash-preview-09-2025",
                             google_api_key=os.getenv("GEMINI_API_KEY"),
                             convert_system_message_to_human=True)


# --- 2. CONFIGURA칂츾O DO BACKEND (Mem칩ria Tempor치ria - Modo Moderno) ---

# Fun칞칚o para buscar o hist칩rico da mem칩ria (usando st.session_state)
# Esta fun칞칚o 칠 exigida pelo RunnableWithMessageHistory
def get_session_history(session_id):
    if session_id not in st.session_state.chat_histories:
        st.session_state.chat_histories[session_id] = ChatMessageHistory()
    return st.session_state.chat_histories[session_id]

# Inicializa o armazenamento de hist칩ricos no session_state, se n칚o existir
if "chat_histories" not in st.session_state:
    st.session_state.chat_histories = {}

# Cria o template do prompt, incluindo um placeholder para o hist칩rico
prompt_template = ChatPromptTemplate.from_messages(
    [
        ("system", "Voc칡 칠 um assistente prestativo. Responda 맙 perguntas do usu치rio."),
        MessagesPlaceholder(variable_name="history"), # Onde o hist칩rico ser치 inserido
        ("human", "{input}"), # Onde a pergunta do usu치rio ser치 inserida
    ]
)

# Cria a "Chain" moderna com mem칩ria
# Esta chain agora sabe como buscar o hist칩rico usando get_session_history
chain_with_memory = RunnableWithMessageHistory(
    prompt_template | llm, # Conecta o prompt ao LLM
    get_session_history,   # Fun칞칚o que fornece o hist칩rico correto
    input_messages_key="input", # Nome da vari치vel da entrada do usu치rio no prompt
    history_messages_key="history", # Nome da vari치vel do hist칩rico no prompt
)


# --- 3. CONFIGURA칂츾O DO FRONTEND (Streamlit - 1 hora) ---
st.set_page_config(page_title="Chatbot Funcional v2", layout="wide")
st.title("Meu Chatbot com Gemini (v2) 游뱄")

# Define um ID de sess칚o padr칚o (poderia ser o ID do usu치rio, etc.)
# Para este exemplo simples, usamos um ID fixo "chat_principal"
session_id = "chat_principal"

# Exibe o hist칩rico da conversa (lendo da mem칩ria vinculada  session_id)
chat_history = get_session_history(session_id)
for message in chat_history.messages:
    role = "ai" if isinstance(message, AIMessage) else "human"
    with st.chat_message(role):
        st.markdown(message.content)

# Pega a nova mensagem do usu치rio
if prompt := st.chat_input("Digite sua mensagem..."):
    
    # 1. (Frontend) Mostra a mensagem do usu치rio na tela
    with st.chat_message("human"):
        st.markdown(prompt)

    # 2. (Backend/LLM) Pensa na resposta E SALVA O HIST칍RICO
    # Usamos "invoke" na chain moderna, passando o input e a config (session_id)
    # A chain_with_memory automaticamente busca o hist칩rico, chama o LLM e salva a nova resposta
    response = chain_with_memory.invoke(
        {"input": prompt},
        config={"configurable": {"session_id": session_id}} 
    )

    # 3. (Frontend) Mostra a resposta da IA na tela
    # A resposta agora est치 dentro do objeto response, no atributo 'content'
    with st.chat_message("ai"):
        st.markdown(response.content)
